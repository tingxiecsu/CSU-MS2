batch_size: 800
epochs: 4000
eval_every_n_epochs: 5
smilesmodel_finetune: False
smilesmodel_finetune_path: "/public/home/hpc212307003/molclr_cnn/nist_w2v/pretrained_gin/checkpoints/model.pth"
fine_tune_from: "/home/xieting/graph_transformer_esa_hpc/runs/dem4_esa_casmi22_40/"
log_every_n_steps: 2
learning_rate: 5e-06
weight_decay: 0.0001
fp16_precision: True
truncation: True

model_config:
  num_layer: 5
  emb_dim: 300
  feat_dim: 512
  drop_ratio: 0.3
  pool : mean
  spec_embed_dim: 256
  dropout: 0.1
  layers: 3
  embed_dim: 256
  
dataset:
  s: 1
  num_workers: 0
  valid_size: 0.1
  ms2_file: "/home/xieting/graph_transformer_esa_hpc/data/qotf_40.mgf"
  smi_file: "/home/xieting/graph_transformer_esa_hpc/data/smi_qtof_40.npy"
loss:
  temperature: 0.1
  use_cosine_similarity: True
  alpha_weight: 0.75
