batch_size: 1050
epochs: 300
eval_every_n_epochs: 5
smilesmodel_finetune: False
smilesmodel_finetune_path: "/public/home/hpc212307003/molclr_cnn/nist_w2v/pretrained_gin/checkpoints/model.pth"
fine_tune_from: "/home/xieting/graph_transformer_esa/runs/Jul09_10-57-28_ubuntu/"
log_every_n_steps: 2
learning_rate: 1e-06
weight_decay: 0.0001
fp16_precision: True
truncation: True

model_config:
  num_layer: 5
  emb_dim: 300
  feat_dim: 512
  drop_ratio: 0.3
  pool : mean
  spec_embed_dim: 256
  dropout: 0.1
  layers: 3
  embed_dim: 256
  
dataset:
  s: 1
  num_workers: 0
  valid_size: 0.2
  ms2_file: "/home/xieting/cl_data/train3_hcd_posh_40.mgf"
  smi_file: "/home/xieting/cl_data/train3_hcd_posh_40_smi.npy"

loss:
  temperature: 0.1
  use_cosine_similarity: True
  alpha_weight: 0.75
